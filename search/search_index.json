{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quickstart Guide","text":""},{"location":"#installation","title":"Installation","text":"<p>To install StateDict2PyTree, run:</p> <pre><code>pip install statedict2pytree\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>StateDict2PyTree provides two ways to convert PyTorch state dicts to JAX pytrees, depending on if the model fits in your memory or not.</p> <p>The main function is the <code>convert</code> function, which can be imported from <code>from statedict2pytree.converter import convert</code> and has these arguments:</p> <pre><code>def convert(\n    from_memory: Optional[bool] = False,\n    from_path: Optional[bool] = False,\n    state_dict: Optional[dict[str, torch.Tensor]] = None,\n    pytree: Optional[PyTree] = None,\n    path_to_state_dict_object: Optional[str] = None,\n    path_to_pytree_object: Optional[str] = None,\n    chunkify: bool = False,\n    path_to_state_dict_chunks: Optional[str] = None,\n    path_to_pytree_chunks: Optional[str] = None,\n    arrange_with_anthropic: bool = False,\n    anthropic_model: Literal[\"haiku\", \"opus\", \"sonnet\", \"sonnet3.5\"] = \"sonnet3.5\",\n    target_name: str = \"model.eqx\",\n    target_dir: str = \".\",\n):\n    ...\n</code></pre> <p>Depending on which mode you're using (<code>from_memory</code> or <code>from_path</code>), different arguments become required. To transform from memory, you need to provide these arguments:</p> <pre><code>    state_dict: dict[str, torch.Tensor],\n    pytree: PyTree,\n    arrange_with_anthropic: bool,\n    anthropic_model: str,\n    target_dir: str,\n    target_name: str,\n</code></pre> <p>(Note that the default parameters are applied)</p> <p>If your model does not fit on memory, you would use <code>from_path=True</code>. Here, the program splits into 2 paths: either you chunkify the model yourself and store them somewhere and provide the path, or you let the program chunkify your models in a <code>tempdir</code> (meaning the chunks are deleted afterwards).</p> <p>You will need these arguments in that case: <pre><code>path_to_state_dict_chunks: Optional[str],\npath_to_state_dict_object: Optional[str],\nchunkify: bool,\npath_to_pytree_chunks: Optional[str],\npath_to_pytree_object: Optional[str],\narrange_with_anthropic: bool,\nanthropic_model: str,\ntarget_dir: str,\ntarget_name: str,\n</code></pre></p> <p>What's important here is to know if your models weights and the weights in the state dict are in the right order! This might not always be the case.</p> <p>For example, in this case, the conversion will fail:</p> <p>PyTree: lin1 - Shape: 10, 20 lin2 - Shape: 20, 20</p> <p>State Dict: lin2 - Shape: 20, 20 lin1 - Shape: 10, 20</p> <p>This will fail, because <code>s2p</code> will look at the order of the weights and in this case, the shapes don't match.</p> <p>You might be wondering now: \"why not just match the shapes?\" Unfortunately, it's not guaranteed that the shapes are unique. Consider this case:</p> <p>PyTree: lin1 - Shape: 10, 10 lin2 - Shape: 10, 10 lin3 - Shape: 10, 10</p> <p>State Dict: lin3 - Shape: 10, 10 lin2 - Shape: 10, 10 lin1 - Shape: 10, 10</p> <p>Conversion in this case would succeed, but the resulting model would be incorrect. We could try to infer the order with the names, but after trying a couple of algorithms to match the names, nothing really worked. This is why you can either:</p> <ul> <li>use the provided GUI to align the weights yourself</li> <li>or use Anthropic's Claude model to align them, without starting the GUI</li> </ul> <p>To start the GUI, you will need to do the following:</p> <pre><code>from statedict2pytree.app import start_conversion_from_pytree_and_state_dict\nfrom statedict2pytree.app import start_conversion_from_paths\n\n# if in memory:\nstart_conversion_from_pytree_and_state_dict(\n    pytree: PyTree, state_dict: dict[str, torch.Tensor]\n)\n\n# else:\nstart_conversion_from_paths(pytree_path: str, state_dict_path: str)\n</code></pre> <p>This will start a Flask server and you can start aligning your model.</p> <p>On the other hand, you can also provide your <code>ANTHROPIC_API_KEY</code> in your environment variables and set <code>arrange_with_anthropic</code> to <code>True</code> in the main <code>convert</code> function. It will ask you to confirm if you like the arrangement it made (usually it works on the first try).</p>"},{"location":"examples/","title":"Usage Examples","text":""},{"location":"examples/#converting-llama-3-model","title":"Converting LLaMA 3 Model","text":"<p>This example demonstrates how to convert the LLaMA 3 model from PyTorch to JAX: (see the <code>examples</code> directory on GitHub)</p> <pre><code>import json\nimport os\nimport tempfile\n\nimport jax\nimport torch\nfrom jaxonmodels.transformers.llama.llama3 import LLaMA\nfrom jaxonmodels.transformers.llama.model_args import LLaMAModelArgs\nfrom loguru import logger\nfrom memory_profiler import profile\nfrom statedict2pytree.app import start_conversion_from_paths\nfrom statedict2pytree.utils.utils_pytree import chunkify_pytree, serialize_pytree_chunks\nfrom statedict2pytree.utils.utils_state_dict import chunkify_state_dict\nfrom tqdm import tqdm\n\n\n@profile\ndef main():\n    with tempfile.TemporaryDirectory() as tmp:\n        logger.info(\"Loading state dict...\")\n        state_dict = torch.load(\"Meta-Llama-3-8B/consolidated.00.pth\")\n        logger.info(\"Done. Converting to float16...\")\n        for key in tqdm(state_dict.keys()):\n            if torch.is_tensor(state_dict[key]):\n                state_dict[key] = state_dict[key].to(torch.float16)\n        logger.info(\"Done. Chunkifying state dict...\")\n        chunkify_state_dict(state_dict, tmp)\n        logger.info(\"Done. Deleting state dict...\")\n        del state_dict\n        with open(\"Meta-Llama-3-8B/params.json\", \"r\") as f:\n            params = json.load(f)\n\n        model_args = LLaMAModelArgs(**params)\n        model_args.precision = \"quarter\"\n        key = jax.random.PRNGKey(21)\n        logger.info(\"Creating JAX model...\")\n        model = LLaMA(model_args, key=key)\n        logger.info(\"Done. Chunkifying PyTree...\")\n        paths = chunkify_pytree(model, tmp)\n        logger.info(\"Done. Starting server...\")\n        start_conversion_from_paths(tmp, tmp)\n        # autoconvert_from_paths(pytree_path=tmp, state_dict_path=tmp)\n        paths = os.listdir(f\"{tmp}/pytree\")\n        tree_paths = []\n        for p in paths:\n            tree_paths.append(f\"{tmp}/pytree/\" + p)\n        logger.info(\"SERIALIZING PYTREE\")\n        serialize_pytree_chunks(model, tree_paths, \"model.eqx\")\n        logger.info(\"Done.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/#converting-resnet18-model","title":"Converting ResNet18 Model","text":"<p>This example shows how to convert a ResNet18 model from PyTorch to JAX:</p> <p>(see the <code>examples</code> directory on GitHub)</p> <pre><code>import jax\nimport statedict2pytree.app as s2p\nfrom resnet_model import resnet18\nfrom torchvision.models import resnet18 as t_resnet18, ResNet18_Weights\n\ndef convert_resnet():\n    resnet_jax = resnet18(key=jax.random.PRNGKey(33), make_with_state=False)\n    resnet_torch = t_resnet18(weights=ResNet18_Weights.DEFAULT)\n    state_dict = resnet_torch.state_dict()\n    s2p.start_conversion_from_pytree_and_state_dict(resnet_jax, state_dict)\n\nif __name__ == \"__main__\":\n    convert_resnet()\n</code></pre>"}]}